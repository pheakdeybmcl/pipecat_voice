PRODUCTION MIGRATION RUNBOOK
============================

Project: FreeSWITCH + AI Voice Agent (fs_engine)
Goal: Move to a new production server and run reliably.

This document includes:
- Step-by-step migration/run instructions
- Current configuration values
- Full contents of key files (for backup/restore)

================================================================================
0) CURRENT ARCHITECTURE (WHAT YOU HAVE NOW)
--------------------------------------------------------------------------------
Components:
1) FreeSWITCH in Docker
   - Image: fs-audio-tts-shout
   - Container: fs-audio-tts
   - Modules: mod_audio_stream, mod_event_socket, mod_shout, mod_tts_commandline
   - Dialplan routes inbound calls to WS and keeps call open.

2) AI server (Python)
   - fs_engine.py (FastAPI + WebSocket)
   - `uvicorn fs_engine:app --host 0.0.0.0 --port 8000`
   - Flow: VAD -> STT (Whisper) -> LLM (Ollama) -> TTS (Piper/Edge) -> playback via ESL

3) LLM
   - Ollama server on same host
   - Model set via .env: OLLAMA_MODEL (ex: qwen3:0.6b or deepseek-r1:1.5b)

4) STT
   - faster-whisper
   - .env: WHISPER_SIZE=small (CPU) or large-v3 (quality)

5) TTS
   - Piper (offline) with Edge fallback
   - .env: TTS_BACKEND=piper + PIPER_MODEL_MAP/PIPER_CONFIG_MAP

6) Config
   - .env and company_profile.json are the source of truth.

================================================================================
1) NEW PRODUCTION SERVER REQUIREMENTS
--------------------------------------------------------------------------------
- Ubuntu 22.04+ (or Debian 12)
- CPU: 8+ cores recommended (Whisper CPU)
- RAM: 16GB+ (more if using large-v3)
- Storage: 50GB+ (models + recordings)
- Network: stable LAN, optional public IP for SIP

================================================================================
2) FREE SWITCH (FS) BUILD + RUN (DOCKER)
--------------------------------------------------------------------------------
2.1 Build image

  cd /opt/pipecat
  docker build -f Dockerfile.freeswitch -t fs-audio-tts-shout .

2.2 Run container

  docker run -d --name fs-audio-tts --network host fs-audio-tts-shout

2.3 Reload FS XML after dialplan edits

  docker exec -it fs-audio-tts /usr/local/freeswitch/bin/fs_cli -p ClueCon -x "reloadxml"

2.4 Check modules loaded

  docker exec -it fs-audio-tts /usr/local/freeswitch/bin/fs_cli -p ClueCon -x "show modules" | grep -E "mod_audio_stream|mod_shout|mod_tts_commandline"

================================================================================
3) DIALPLAN (CALL ROUTING)
--------------------------------------------------------------------------------
Create (or update) /usr/local/freeswitch/etc/freeswitch/dialplan/default/0000000000.xml:

<include>
  <extension name="test_0000000000">
    <condition field="destination_number" expression="^0000000000$">
      <action application="answer"/>

      <action application="set" data="media_timeout=0"/>
      <action application="set" data="media_hold_timeout=0"/>
      <action application="set" data="rtp_timeout_sec=0"/>
      <action application="set" data="rtp_hold_timeout_sec=0"/>
      <action application="set" data="session_timeout=0"/>
      <action application="set" data="park_timeout=0"/>

      <action application="set" data="hold_music=local_stream://silence"/>
      <action application="set" data="ringback=local_stream://silence"/>

      <action application="set" data="STREAM_SUPPRESS_LOG=true"/>
      <action application="set" data="STREAM_BUFFER_SIZE=20"/>
      <action application="set" data="api_result=${bgapi(uuid_audio_stream ${uuid} start ws://YOUR_SERVER_IP:8000/ws_fs?session_id=${uuid} mono 16k)}"/>

      <action application="sleep" data="3600000"/>
    </condition>
  </extension>
</include>

IMPORTANT: Replace YOUR_SERVER_IP with your host IP (e.g. 192.168.1.223).

================================================================================
4) PYTHON AI SERVER (fs_engine)
--------------------------------------------------------------------------------
4.1 Create venv and install deps

  cd /opt/pipecat
  python3 -m venv .venv
  source .venv/bin/activate
  pip install -r requirements.txt

4.2 Run server

  source .venv/bin/activate
  uvicorn fs_engine:app --host 0.0.0.0 --port 8000

================================================================================
5) OLLAMA (LLM)
--------------------------------------------------------------------------------
5.1 Install ollama
  - https://ollama.com/download

5.2 Pull models
  ollama pull qwen3:0.6b
  ollama pull deepseek-r1:1.5b

5.3 Verify
  curl http://localhost:11434/api/tags

================================================================================
6) PIPER TTS (OFFLINE)
--------------------------------------------------------------------------------
6.1 Install Piper binary
  - Download from: https://github.com/rhasspy/piper/releases/latest
  - Extract to /usr/local/share/piper and /usr/local/bin/piper

6.2 Ensure piper_wrapper.sh points to correct libs
  - /usr/local/share/piper must include libpiper_phonemize.so

6.3 Download voices
  - https://huggingface.co/rhasspy/piper-voices

Example:
  en_US-lessac-medium.onnx + .json
  hi_IN-priyamvada-medium.onnx + .json

6.4 Update .env to point to models

================================================================================
7) SYSTEMD (OPTIONAL)
--------------------------------------------------------------------------------
Create /etc/systemd/system/fs-engine.service:

[Unit]
Description=fs_engine (AI Voice Agent)
After=network.target

[Service]
User=smith
WorkingDirectory=/opt/pipecat
EnvironmentFile=/opt/pipecat/.env
ExecStart=/opt/pipecat/.venv/bin/uvicorn fs_engine:app --host 0.0.0.0 --port 8000
Restart=always
RestartSec=2

[Install]
WantedBy=multi-user.target

Then:
  sudo systemctl daemon-reload
  sudo systemctl enable --now fs-engine
  sudo systemctl status fs-engine

================================================================================
8) TROUBLESHOOTING CHECKLIST
--------------------------------------------------------------------------------
- ESL connection refused: make sure FS is running and port 8021 is open.
- ACK Timeout: SIP device not sending ACK (check Zoiper port + IP).
- No audio in TTS: verify Piper model/config paths and `piper` libs.
- Too many short utterances: increase MIN_UTTERANCE_MS and VAD thresholds.

================================================================================
9) FILE BACKUPS (FULL CONTENTS)
--------------------------------------------------------------------------------
Below are the full contents of the key files used in production.


=== FILE: fs_engine.py ===
# fs_engine.py
# FreeSWITCH mod_audio_stream WS server with:
# - VAD utterance segmentation
# - STT (faster-whisper)
# - LLM (Ollama OpenAI-compatible /v1)
# - TTS (edge_tts -> wav/mp3)
# - Optional ESL listener to auto-play streamAudio responses

import asyncio
import base64
import json
import logging
import os
import re
import shutil
import subprocess
import time
import uuid
from urllib.parse import unquote
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
import webrtcvad
from fastapi import FastAPI, WebSocket
from starlette.websockets import WebSocketDisconnect
from faster_whisper import WhisperModel
from openai import AsyncOpenAI
import edge_tts

# ---------------- Config ----------------
# Load .env (optional) before reading environment variables
def _load_env_file(path: str) -> None:
    if not path or not os.path.exists(path):
        return
    try:
        entries: dict[str, str] = {}
        with open(path, "r", encoding="utf-8") as f:
            for raw in f:
                line = raw.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith("export "):
                    line = line[len("export ") :].lstrip()
                if "=" not in line:
                    continue
                key, val = line.split("=", 1)
                key = key.strip()
                val = val.strip().strip('"').strip("'")
                if key:
                    entries[key] = val
        override = entries.get("FS_ENGINE_ENV_OVERRIDE", "").lower() in ("1", "true", "yes")
        for key, val in entries.items():
            if override or key not in os.environ:
                os.environ[key] = val
    except Exception:
        # Best effort: if parsing fails, fall back to existing env vars.
        return


_load_env_file(os.getenv("FS_ENGINE_ENV", ".env"))

def _load_company_profile(path: str) -> dict:
    if not path:
        return {}
    try:
        if not os.path.exists(path):
            return {}
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


def _format_company_profile(profile: dict) -> str:
    if not profile:
        return ""
    lines: list[str] = []
    name = profile.get("company_name")
    if name:
        lines.append(f"Company: {name}")
    tagline = profile.get("tagline")
    if tagline:
        lines.append(f"Tagline: {tagline}")
    services = profile.get("services") or []
    if services:
        lines.append("Services:")
        for svc in services:
            title = svc.get("name", "Service")
            desc = svc.get("description", "")
            if desc:
                lines.append(f"- {title}: {desc}")
            else:
                lines.append(f"- {title}")
            feats = svc.get("features") or []
            if feats:
                lines.append(f"  Features: {', '.join(feats)}")
    plans = profile.get("plans") or []
    if plans:
        lines.append("Plans:")
        for plan in plans:
            pname = plan.get("name", "Plan")
            price = plan.get("price", "")
            includes = plan.get("includes", "")
            summary = pname
            if price:
                summary += f" ({price})"
            if includes:
                summary += f" — {includes}"
            lines.append(f"- {summary}")
    support = profile.get("support") or {}
    if support:
        lines.append("Support:")
        if support.get("hours"):
            lines.append(f"- Hours: {support['hours']}")
        if support.get("email"):
            lines.append(f"- Email: {support['email']}")
        if support.get("phone"):
            lines.append(f"- Phone: {support['phone']}")
    return "\n".join(lines)


def _matches_allowed_topics(text: str, topics: list[str]) -> bool:
    if not topics:
        return True
    t = text.lower()
    return any(topic in t for topic in topics)


# Ollama (OpenAI-compatible)
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1")
MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
LLM_MAX_CONCURRENCY = max(1, int(os.getenv("LLM_MAX_CONCURRENCY", "4")))
LLM_SEMAPHORE = asyncio.Semaphore(LLM_MAX_CONCURRENCY)
llm_client = AsyncOpenAI(api_key="ollama", base_url=OLLAMA_BASE_URL)

STT_MAX_CONCURRENCY = max(1, int(os.getenv("STT_MAX_CONCURRENCY", "2")))
STT_SEMAPHORE = asyncio.Semaphore(STT_MAX_CONCURRENCY)

BASE_SYSTEM_PROMPT = os.getenv(
    "SYSTEM_PROMPT",
    "You are an assistant for this app and a professional call center agent.\n"
    "Respond like a human in a live phone call.\n"
    "\n"
    "Rules:\n"
    "- Match the length of your reply to the user's intent.\n"
    "- If the user gives a short or unclear input, respond briefly or ask a clarifying question.\n"
    "- Do not ask follow-up questions or offer extra help unless it's required to answer.\n"
    "- Do not over-explain unless the user asks for details.\n"
    "- Stop speaking as soon as your point is made.\n"
)
COMPANY_PROFILE_PATH = os.getenv("COMPANY_PROFILE_PATH", "")
COMPANY_PROFILE = _load_company_profile(COMPANY_PROFILE_PATH)
PROFILE_TEXT = _format_company_profile(COMPANY_PROFILE)
_company_name = COMPANY_PROFILE.get("company_name", "").strip() if isinstance(COMPANY_PROFILE, dict) else ""
_company_aliases = []
if isinstance(COMPANY_PROFILE, dict):
    _company_aliases = [a for a in (COMPANY_PROFILE.get("aliases") or []) if isinstance(a, str)]
ENV_COMPANY_ALIASES = [a.strip() for a in os.getenv("COMPANY_ALIASES", "").split(",") if a.strip()]
COMPANY_ALIASES = [a for a in (_company_aliases + ENV_COMPANY_ALIASES) if a]
COMPANY_RULES = (
    "Company-only policy:\n"
    "- Only answer questions related to the company, its services, plans, pricing, support, or account issues.\n"
    "- If a request is unrelated, politely refuse and redirect to company topics.\n"
)
if PROFILE_TEXT:
    SYSTEM_PROMPT = BASE_SYSTEM_PROMPT + "\n\n" + COMPANY_RULES + "\nCompany profile:\n" + PROFILE_TEXT + "\n"
else:
    SYSTEM_PROMPT = BASE_SYSTEM_PROMPT + "\n\n" + COMPANY_RULES

# Optional greeting (no LLM)
GREETING_ENABLED = os.getenv("GREETING_ENABLED", "false").lower() in ("1", "true", "yes")
GREETING_TEXT = os.getenv("GREETING_TEXT", "").strip()
if not GREETING_TEXT and isinstance(COMPANY_PROFILE, dict):
    GREETING_TEXT = str(COMPANY_PROFILE.get("greeting", "")).strip()
if GREETING_TEXT and _company_name:
    try:
        GREETING_TEXT = GREETING_TEXT.format(company_name=_company_name)
    except Exception:
        pass

ALLOWED_TOPICS = [t.strip().lower() for t in os.getenv("ALLOWED_TOPICS", "").split(",") if t.strip()]
OFFTOPIC_RESPONSE = os.getenv(
    "OFFTOPIC_RESPONSE",
    "I can help with BigfootMediaTech services like voice, messaging, eSIM, plans, or support. How can I assist?",
)
OFFTOPIC_RESPONSE_SHORT = os.getenv(
    "OFFTOPIC_RESPONSE_SHORT",
    "Please say one of: voice, messaging, eSIM, plans, billing, or support.",
)
OFFTOPIC_REPEAT_LIMIT = max(1, int(os.getenv("OFFTOPIC_REPEAT_LIMIT", "2")))

# Audio input expected from FreeSWITCH: PCM16 little-endian, mono.
SAMPLE_RATE = int(os.getenv("FS_SAMPLE_RATE", "16000"))
SAMPLE_WIDTH = 2  # bytes per sample (PCM16)
FRAME_MS = 20     # VAD supports only 10/20/30ms
FRAME_SAMPLES = int(SAMPLE_RATE * FRAME_MS / 1000)
FRAME_BYTES = FRAME_SAMPLES * SAMPLE_WIDTH

MIN_UTTERANCE_MS = int(os.getenv("MIN_UTTERANCE_MS", "400"))
MIN_UTTERANCE_FRAMES = max(1, int(MIN_UTTERANCE_MS / FRAME_MS))
MIN_UTTERANCE_BYTES = MIN_UTTERANCE_FRAMES * FRAME_BYTES
MIN_UTTERANCE_RMS = float(os.getenv("MIN_UTTERANCE_RMS", "0.001"))

VAD_MODE = int(os.getenv("VAD_MODE", "2"))  # 0-3 (3 = most aggressive)
MAX_UTTERANCE_SECONDS = float(os.getenv("MAX_UTTERANCE_SECONDS", "12.0"))
END_SILENCE_MS = int(os.getenv("END_SILENCE_MS", "2000"))
END_SILENCE_FRAMES = max(1, int(END_SILENCE_MS / FRAME_MS))
START_SPEECH_FRAMES = max(1, int(os.getenv("START_SPEECH_FRAMES", "3")))
VAD_RMS_THRESHOLD = float(os.getenv("VAD_RMS_THRESHOLD", "0.0025"))
NOISE_FLOOR_ALPHA = float(os.getenv("NOISE_FLOOR_ALPHA", "0.95"))
NOISE_FLOOR_MULT = float(os.getenv("NOISE_FLOOR_MULT", "2.5"))
ALLOW_BARGE_IN = os.getenv("ALLOW_BARGE_IN", "false").lower() in ("1", "true", "yes")
LISTEN_DURING_TTS = os.getenv("LISTEN_DURING_TTS", "false").lower() in ("1", "true", "yes")
MIN_WORDS = max(1, int(os.getenv("MIN_WORDS", "1")))
MIN_AVG_LOGPROB = float(os.getenv("MIN_AVG_LOGPROB", "-1.0"))
MAX_NO_SPEECH_PROB = float(os.getenv("MAX_NO_SPEECH_PROB", "0.6"))
MIN_WORDS_NON_EN = max(1, int(os.getenv("MIN_WORDS_NON_EN", "1")))
MIN_AVG_LOGPROB_NON_EN = float(os.getenv("MIN_AVG_LOGPROB_NON_EN", "-1.6"))
MAX_NO_SPEECH_PROB_NON_EN = float(os.getenv("MAX_NO_SPEECH_PROB_NON_EN", "0.9"))
TTS_CHARS_PER_SECOND = float(os.getenv("TTS_CHARS_PER_SECOND", "14.0"))
POST_TTS_GRACE_MS = int(os.getenv("POST_TTS_GRACE_MS", "600"))
TTS_MIN_SUPPRESS_MS = int(os.getenv("TTS_MIN_SUPPRESS_MS", "0"))
MAX_TTS_SUPPRESS_SEC = float(os.getenv("MAX_TTS_SUPPRESS_SEC", "30.0"))

# Language / multilingual
STT_LANGUAGE = os.getenv("STT_LANGUAGE", "en")  # "en" or "auto"
LANG_LOCK_MIN_PROB = float(os.getenv("LANG_LOCK_MIN_PROB", "0.70"))
LANG_FALLBACK = os.getenv("LANG_FALLBACK", "en")

# IVR language selection
IVR_LANG_SELECT_ENABLED = os.getenv("IVR_LANG_SELECT_ENABLED", "false").lower() in ("1", "true", "yes")
IVR_LANG_TIMEOUT_MS = int(os.getenv("IVR_LANG_TIMEOUT_MS", "10000"))
IVR_LANG_CONFIRM = os.getenv("IVR_LANG_CONFIRM", "false").lower() in ("1", "true", "yes")
IVR_LANG_PROMPT = os.getenv(
    "IVR_LANG_PROMPT",
    "For English, press 1. For Hindi, press 2.",
)

IVR_LANG_WELCOME_EN = os.getenv("IVR_LANG_WELCOME_EN", "Welcome. How can I help you today?")
IVR_LANG_WELCOME_HI = os.getenv("IVR_LANG_WELCOME_HI", "स्वागत है। मैं आपकी कैसे मदद कर सकती हूँ?")
IVR_SPEECH_ENABLED = os.getenv("IVR_SPEECH_ENABLED", "false").lower() in ("1", "true", "yes")
IVR_SPEECH_MAX_WORDS = max(1, int(os.getenv("IVR_SPEECH_MAX_WORDS", "4")))
IVR_SPEECH_MAP_RAW = os.getenv("IVR_SPEECH_MAP", "en=english,eng|hi=hindi,हिंदी,हिन्दी")

MERGE_WINDOW_MS = int(os.getenv("MERGE_WINDOW_MS", "2000"))
MERGE_PREV_WORDS_MAX = max(1, int(os.getenv("MERGE_PREV_WORDS_MAX", "6")))
MERGE_MAX_CHARS = max(64, int(os.getenv("MERGE_MAX_CHARS", "400")))


def _parse_lang_map(raw: str) -> dict[str, str]:
    mapping: dict[str, str] = {}
    if not raw:
        return mapping
    for item in raw.split(","):
        item = item.strip()
        if not item or "=" not in item:
            continue
        k, v = item.split("=", 1)
        k = k.strip()
        v = v.strip().lower()
        if k and v:
            mapping[k] = v
    return mapping


IVR_LANG_MAP = _parse_lang_map(os.getenv("IVR_LANG_MAP", "1=en,2=hi"))


def _parse_ivr_speech_map(raw: str) -> dict[str, list[str]]:
    mapping: dict[str, list[str]] = {}
    if not raw:
        return mapping
    for part in raw.split("|"):
        part = part.strip()
        if not part or "=" not in part:
            continue
        lang, phrases = part.split("=", 1)
        lang = lang.strip().lower()
        if not lang:
            continue
        vals = [p.strip().lower() for p in phrases.split(",") if p.strip()]
        if vals:
            mapping[lang] = vals
    return mapping


IVR_SPEECH_MAP = _parse_ivr_speech_map(IVR_SPEECH_MAP_RAW)


def _match_ivr_speech_choice(text: str) -> str:
    if not text:
        return ""
    t = text.strip().lower()
    if not t:
        return ""
    words = [w for w in re.split(r"\s+", t) if w]
    if len(words) > IVR_SPEECH_MAX_WORDS:
        return ""
    for lang, phrases in IVR_SPEECH_MAP.items():
        for phrase in phrases:
            if phrase and phrase in t:
                return lang
    return ""


def _normalize_company_terms(text: str) -> str:
    if not text or not COMPANY_ALIASES or not _company_name:
        return text
    out = text
    for alias in COMPANY_ALIASES:
        alias = alias.strip()
        if not alias:
            continue
        try:
            out = re.sub(rf"\b{re.escape(alias)}\b", _company_name, out, flags=re.IGNORECASE)
        except re.error:
            out = out.replace(alias, _company_name)
    return out
DEFAULT_LANGUAGE = os.getenv("DEFAULT_LANGUAGE", "en")

def _parse_lang_voice_map(raw: str) -> dict:
    mapping: dict[str, str] = {}
    for part in (raw or "").split(","):
        if not part.strip():
            continue
        if "=" not in part:
            continue
        k, v = part.split("=", 1)
        k = k.strip().lower()
        v = v.strip()
        if k and v:
            mapping[k] = v
    return mapping


LANG_VOICE_MAP = _parse_lang_voice_map(os.getenv("LANG_VOICE_MAP", ""))

LANG_NAME_MAP = {
    "en": "English",
    "km": "Khmer",
    "hi": "Hindi",
    "th": "Thai",
    "zh": "Chinese",
}


def _lang_name(code: str) -> str:
    return LANG_NAME_MAP.get(code.lower(), code)


def _voice_for_lang(code: str) -> str:
    if not code:
        return DEFAULT_TTS_VOICE
    key = code.lower()
    if key in LANG_VOICE_MAP:
        return LANG_VOICE_MAP[key]
    base = key.split("-", 1)[0]
    if base in LANG_VOICE_MAP:
        return LANG_VOICE_MAP[base]
    return DEFAULT_TTS_VOICE


def _apply_language_choice(state: "SessionState", lang: str) -> None:
    if not lang:
        return
    state.language = lang
    state.language_prob = 1.0
    state.language_locked = True
    state.voice = _voice_for_lang(lang)
    state.lang_select_active = False
    state.lang_selected = True
    LOG.info("[call %s] IVR language selected: %s -> voice=%s", state.session_id, lang, state.voice)


async def _flush_utter_q(q: Optional[asyncio.Queue]) -> None:
    if not q:
        return
    try:
        while True:
            q.get_nowait()
    except Exception:
        return


async def _handle_language_switch(state: "SessionState", lang: str) -> None:
    if not lang or not state.ws:
        return
    # Stop any in-flight tasks and clear state
    state.interrupted = True
    await cancel_task(state.speaking_task, "tts")
    await cancel_task(state.llm_task, "llm")
    await cancel_task(state.mic_unmute_task, "mic_unmute")
    state.speaking_task = None
    state.llm_task = None
    state.mic_unmute_task = None
    state.suppress_until = 0.0
    state.block_listen = False
    state.history.clear()
    state.offtopic_count = 0
    state.turn_id += 1
    await _flush_utter_q(state.utter_q)

    _apply_language_choice(state, lang)
    welcome = IVR_LANG_WELCOME_EN
    if lang.lower().startswith("hi"):
        welcome = IVR_LANG_WELCOME_HI
    try:
        await play_tts_text(state.ws, state, welcome)
    except Exception as e:
        LOG.warning("[call %s] IVR welcome failed: %s", state.session_id, e)


def _piper_paths_for_lang(code: str) -> tuple[str, str]:
    if not code:
        code = ""
    key = code.lower()
    base = key.split("-", 1)[0] if key else ""
    model = ""
    config = ""
    if key in PIPER_MODEL_MAP:
        model = PIPER_MODEL_MAP[key]
    elif base and base in PIPER_MODEL_MAP:
        model = PIPER_MODEL_MAP[base]
    else:
        model = PIPER_MODEL

    if key in PIPER_CONFIG_MAP:
        config = PIPER_CONFIG_MAP[key]
    elif base and base in PIPER_CONFIG_MAP:
        config = PIPER_CONFIG_MAP[base]
    else:
        config = PIPER_CONFIG

    if not config and model:
        guess = model + ".json"
        if os.path.exists(guess):
            config = guess
    return model, config

# TTS
SERVER_TTS_ENABLED = os.getenv("SERVER_TTS_ENABLED", "true").lower() in ("1", "true", "yes")
DEFAULT_TTS_VOICE = os.getenv("TTS_VOICE", "en-US-JennyNeural")
MAX_TTS_CHARS = int(os.getenv("MAX_TTS_CHARS", "480"))
TTS_AUDIO_TYPE = os.getenv("TTS_AUDIO_TYPE", "wav").lower()
EDGE_OUTPUT_FORMAT = os.getenv("EDGE_OUTPUT_FORMAT", "riff-16khz-16bit-mono-pcm")
TTS_WAV_SAMPLE_RATE = int(os.getenv("TTS_WAV_SAMPLE_RATE", "8000"))
TTS_RAW_SAMPLE_RATE = int(os.getenv("TTS_RAW_SAMPLE_RATE", "8000"))
USE_FS_TTS = os.getenv("USE_FS_TTS", "false").lower() in ("1", "true", "yes")
FS_TTS_ENGINE = os.getenv("FS_TTS_ENGINE", "tts_commandline")
FS_TTS_VOICE = os.getenv("FS_TTS_VOICE", DEFAULT_TTS_VOICE)
FFMPEG_PATH = shutil.which("ffmpeg")
TTS_BACKEND = os.getenv("TTS_BACKEND", "edge").lower()  # edge | piper
PIPER_BIN = os.getenv("PIPER_BIN", "piper")
PIPER_MODEL = os.getenv("PIPER_MODEL", "")
PIPER_CONFIG = os.getenv("PIPER_CONFIG", "")
PIPER_MODEL_MAP = _parse_lang_voice_map(os.getenv("PIPER_MODEL_MAP", ""))
PIPER_CONFIG_MAP = _parse_lang_voice_map(os.getenv("PIPER_CONFIG_MAP", ""))
PIPER_SPEAKER = os.getenv("PIPER_SPEAKER", "")
PIPER_ESPEAK_DATA = os.getenv("PIPER_ESPEAK_DATA", "/usr/local/share/piper/espeak-ng-data")
PIPER_FALLBACK_EDGE = os.getenv("PIPER_FALLBACK_EDGE", "true").lower() in ("1", "true", "yes")

# Whisper
WHISPER_SIZE = os.getenv("WHISPER_SIZE", "small")  # default model (path or name)
KHMER_WHISPER_PATH = os.getenv("KHMER_WHISPER_PATH", "")
USE_DUAL_WHISPER = bool(KHMER_WHISPER_PATH)

# ESL (auto-play streamAudio responses)
ESL_ENABLED = os.getenv("FS_ESL_ENABLED", "true").lower() in ("1", "true", "yes")
ESL_HOST = os.getenv("FS_ESL_HOST", "127.0.0.1")
ESL_PORT = int(os.getenv("FS_ESL_PORT", "8021"))
ESL_PASSWORD = os.getenv("FS_ESL_PASSWORD", "ClueCon")
ESL_API_CONCURRENCY = max(1, int(os.getenv("FS_ESL_API_CONCURRENCY", "2")))
ESL_API_SEMAPHORE = asyncio.Semaphore(ESL_API_CONCURRENCY)
ESL_TASK: Optional[asyncio.Task] = None

LOG = logging.getLogger("fs_engine")
if not logging.getLogger().handlers:
    logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"), format="%(message)s")
LOG.setLevel(os.getenv("LOG_LEVEL", "INFO").upper())

LOG_COLOR = os.getenv("LOG_COLOR", "true").lower() in ("1", "true", "yes")

def _colorize(text: str, color_code: str) -> str:
    if not LOG_COLOR:
        return text
    return f"\033[{color_code}m{text}\033[0m"

print("[boot] Loading Whisper model...")
whisper_default = WhisperModel(WHISPER_SIZE, device="cpu", compute_type="int8")
whisper_kh = None
if USE_DUAL_WHISPER:
    try:
        whisper_kh = WhisperModel(KHMER_WHISPER_PATH, device="cpu", compute_type="int8")
        print("[boot] Khmer Whisper loaded.")
    except Exception as e:
        whisper_kh = None
        print(f"[boot] Khmer Whisper failed to load: {e}")
print("[boot] Whisper loaded.")

app = FastAPI()
ACTIVE_SESSIONS: dict[str, "SessionState"] = {}


@dataclass
class SessionState:
    session_id: str
    speaking_task: Optional[asyncio.Task] = None
    llm_task: Optional[asyncio.Task] = None
    worker_task: Optional[asyncio.Task] = None
    interrupted: bool = False
    voice: str = DEFAULT_TTS_VOICE
    history: list[dict] = field(default_factory=list)
    turn_id: int = 0
    suppress_until: float = 0.0
    block_listen: bool = False
    mic_muted: bool = False
    mic_muted_reason: str = ""
    mic_unmute_task: Optional[asyncio.Task] = None
    language: str = ""
    language_prob: float = 0.0
    language_locked: bool = False
    offtopic_count: int = 0
    lang_select_active: bool = False
    lang_select_deadline: float = 0.0
    lang_selected: bool = False
    pending_lang: str = ""
    lang_switch_task: Optional[asyncio.Task] = None
    utter_q: Optional[asyncio.Queue] = None
    ws: Optional[WebSocket] = None
    last_user_text: str = ""
    last_user_time: float = 0.0


@dataclass
class UtteranceJob:
    pcm16: Optional[bytes] = None


def pcm16_bytes_to_float32(pcm: bytes) -> np.ndarray:
    a = np.frombuffer(pcm, dtype=np.int16).astype(np.float32)
    a /= 32768.0
    return a


def pcm16_rms(pcm: bytes) -> float:
    if not pcm:
        return 0.0
    a = np.frombuffer(pcm, dtype=np.int16).astype(np.float32)
    a /= 32768.0
    return float(np.sqrt(np.mean(a * a)))


def _clip(text: str, limit: int = 200) -> str:
    if not text:
        return ""
    if len(text) <= limit:
        return text
    return text[:limit] + "..."


def _set_mic_state(state: SessionState, muted: bool, reason: str = "") -> None:
    if muted:
        if not state.mic_muted or state.mic_muted_reason != reason:
            state.mic_muted = True
            state.mic_muted_reason = reason
            msg = f"[call {state.session_id}] ===== MIC OFF ({reason}) ====="
            LOG.warning(_colorize(msg, "1;31"))  # bold red
    else:
        if state.mic_muted:
            state.mic_muted = False
            state.mic_muted_reason = ""
            msg = f"[call {state.session_id}] ===== MIC ON ====="
            LOG.warning(_colorize(msg, "1;32"))  # bold green


def _schedule_mic_unmute(state: SessionState) -> None:
    if LISTEN_DURING_TTS or ALLOW_BARGE_IN:
        return
    if state.mic_unmute_task and not state.mic_unmute_task.done():
        state.mic_unmute_task.cancel()
    delay = max(0.0, state.suppress_until - time.time())
    if delay:
        msg = f"[call {state.session_id}] ===== MIC will unmute in {delay:.2f}s ====="
        LOG.warning(_colorize(msg, "1;33"))  # bold yellow

    async def _unmute_later():
        try:
            if delay:
                await asyncio.sleep(delay)
            if not state.block_listen and time.time() >= state.suppress_until:
                state.suppress_until = 0.0
                _set_mic_state(state, False, "")
        except asyncio.CancelledError:
            return
    state.mic_unmute_task = asyncio.create_task(_unmute_later())


def _estimate_tts_duration_seconds(text: str, audio_type: str, audio_bytes: bytes, sample_rate: int) -> float:
    # Conservative estimate to block mic pickup while TTS plays.
    try:
        if audio_type == "raw":
            dur = len(audio_bytes) / float(sample_rate * SAMPLE_WIDTH) if sample_rate else 0.0
            return max(0.3, min(MAX_TTS_SUPPRESS_SEC, dur))
        if audio_type == "wav":
            import wave
            import io
            with wave.open(io.BytesIO(audio_bytes), "rb") as wf:
                frames = wf.getnframes()
                rate = wf.getframerate() or sample_rate
                channels = wf.getnchannels() or 1
                dur = frames / float(rate) if rate else 0.0
                # Fallback to byte-length estimate when headers are invalid/streaming
                if audio_bytes:
                    pcm_bytes = max(0, len(audio_bytes) - 44)
                    bps = rate * channels * SAMPLE_WIDTH if rate else 0.0
                    dur_from_bytes = (pcm_bytes / bps) if bps else dur
                    if dur <= 0.0 or dur > MAX_TTS_SUPPRESS_SEC * 4 or (dur_from_bytes > 0 and dur > dur_from_bytes * 4):
                        dur = dur_from_bytes
                return max(0.3, min(MAX_TTS_SUPPRESS_SEC, dur))
    except Exception:
        pass
    # Fallback: estimate by text length.
    if not text:
        return 0.5
    return max(0.5, min(MAX_TTS_SUPPRESS_SEC, len(text) / max(8.0, TTS_CHARS_PER_SECOND)))


def stt_whisper_from_pcm16(
    pcm: bytes, lang_hint: str = "", model: Optional[WhisperModel] = None
) -> tuple[str, float, float, str, float]:
    audio = pcm16_bytes_to_float32(pcm)
    stt_model = model or whisper_default
    lang_arg = None
    if lang_hint:
        lang_arg = lang_hint.lower()
    elif STT_LANGUAGE and STT_LANGUAGE.lower() not in ("auto", "none", ""):
        lang_arg = STT_LANGUAGE.lower()
    segments, info = stt_model.transcribe(audio, language=lang_arg)
    texts = []
    avg_logprobs = []
    no_speech_probs = []
    detected_lang = ""
    lang_prob = 0.0
    try:
        if info is not None:
            detected_lang = getattr(info, "language", "") or ""
            lang_prob = float(getattr(info, "language_probability", 0.0) or 0.0)
    except Exception:
        detected_lang = ""
        lang_prob = 0.0
    if lang_arg:
        detected_lang = lang_arg
        if lang_prob <= 0.0:
            lang_prob = 1.0
    for seg in segments:
        texts.append(seg.text)
        if hasattr(seg, "avg_logprob") and seg.avg_logprob is not None:
            avg_logprobs.append(float(seg.avg_logprob))
        if hasattr(seg, "no_speech_prob") and seg.no_speech_prob is not None:
            no_speech_probs.append(float(seg.no_speech_prob))
    text = "".join(texts).strip()
    if not text:
        return "", -99.0, 1.0, detected_lang, lang_prob
    avg_logprob = sum(avg_logprobs) / len(avg_logprobs) if avg_logprobs else -99.0
    no_speech_prob = max(no_speech_probs) if no_speech_probs else 0.0
    return text, avg_logprob, no_speech_prob, detected_lang, lang_prob


def _trim_history(history: list[dict], max_messages: int = 16) -> None:
    if len(history) > max_messages:
        del history[:-max_messages]


async def ollama_reply(state: SessionState, prompt: str) -> str:
    if ALLOWED_TOPICS and not _matches_allowed_topics(prompt, ALLOWED_TOPICS):
        state.offtopic_count += 1
        if state.offtopic_count >= OFFTOPIC_REPEAT_LIMIT:
            return OFFTOPIC_RESPONSE_SHORT
        return OFFTOPIC_RESPONSE
    state.offtopic_count = 0
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    reply_lang = state.language if (state.language_locked and state.language) else (DEFAULT_LANGUAGE or "")
    if reply_lang:
        messages.append(
            {
                "role": "system",
                "content": f"Language: Reply in {_lang_name(reply_lang)} ({reply_lang}).",
            }
        )
    if state.history:
        messages.extend(state.history)
    messages.append({"role": "user", "content": prompt})
    async with LLM_SEMAPHORE:
        resp = await llm_client.chat.completions.create(
            model=MODEL,
            messages=messages,
        )
    msg = resp.choices[0].message.content or ""
    cleaned = msg.strip()
    if prompt:
        state.history.append({"role": "user", "content": prompt})
    if cleaned:
        state.history.append({"role": "assistant", "content": cleaned})
    _trim_history(state.history)
    return cleaned


async def synthesize_tts_mp3(text: str, voice: str) -> bytes:
    tts = edge_tts.Communicate(text=text, voice=voice)
    chunks = []
    async for chunk in tts.stream():
        if chunk["type"] == "audio":
            chunks.append(chunk["data"])
    return b"".join(chunks)


async def synthesize_tts_piper(text: str, lang: str) -> bytes:
    model, config = _piper_paths_for_lang(lang)
    if not model:
        raise RuntimeError("Piper model not configured")
    tmp_wav = None
    try:
        import tempfile
        with tempfile.NamedTemporaryFile(prefix="piper_", suffix=".wav", delete=False) as f:
            tmp_wav = f.name
        cmd = [PIPER_BIN, "--model", model, "--output_file", tmp_wav]
        if config:
            cmd += ["--config", config]
        if PIPER_SPEAKER:
            cmd += ["--speaker", str(PIPER_SPEAKER)]
        if PIPER_ESPEAK_DATA:
            cmd += ["--espeak_data", PIPER_ESPEAK_DATA]
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.DEVNULL,
            stderr=asyncio.subprocess.PIPE,
        )
        _, err = await proc.communicate(input=(text + "\n").encode("utf-8"))
        if proc.returncode != 0:
            raise RuntimeError(f"piper failed: {err.decode('utf-8', errors='ignore')}")
        with open(tmp_wav, "rb") as f:
            return f.read()
    finally:
        if tmp_wav and os.path.exists(tmp_wav):
            try:
                os.unlink(tmp_wav)
            except Exception:
                pass


def _wav_rate_from_format(output_format: str) -> int:
    fmt = (output_format or "").lower()
    if "8khz" in fmt:
        return 8000
    if "24khz" in fmt:
        return 24000
    if "48khz" in fmt:
        return 48000
    return 16000


async def _mp3_to_wav(mp3_bytes: bytes, sample_rate: int) -> bytes:
    if not FFMPEG_PATH:
        raise RuntimeError("ffmpeg not found on PATH")
    proc = await asyncio.create_subprocess_exec(
        FFMPEG_PATH,
        "-hide_banner",
        "-loglevel",
        "error",
        "-i",
        "pipe:0",
        "-acodec",
        "pcm_s16le",
        "-f",
        "wav",
        "-ac",
        "1",
        "-ar",
        str(sample_rate),
        "pipe:1",
        stdin=asyncio.subprocess.PIPE,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    try:
        stdout, stderr = await asyncio.wait_for(proc.communicate(input=mp3_bytes), timeout=20)
    except asyncio.TimeoutError:
        proc.kill()
        await proc.wait()
        raise RuntimeError("ffmpeg convert timed out")
    if proc.returncode != 0:
        raise RuntimeError(f"ffmpeg convert failed: {stderr.decode('utf-8', 'ignore').strip()}")
    return stdout


async def _mp3_to_raw(mp3_bytes: bytes, sample_rate: int) -> bytes:
    if not FFMPEG_PATH:
        raise RuntimeError("ffmpeg not found on PATH")
    proc = await asyncio.create_subprocess_exec(
        FFMPEG_PATH,
        "-hide_banner",
        "-loglevel",
        "error",
        "-i",
        "pipe:0",
        "-f",
        "s16le",
        "-ac",
        "1",
        "-ar",
        str(sample_rate),
        "pipe:1",
        stdin=asyncio.subprocess.PIPE,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    try:
        stdout, stderr = await asyncio.wait_for(proc.communicate(input=mp3_bytes), timeout=20)
    except asyncio.TimeoutError:
        proc.kill()
        await proc.wait()
        raise RuntimeError("ffmpeg convert timed out")
    if proc.returncode != 0:
        raise RuntimeError(f"ffmpeg convert failed: {stderr.decode('utf-8', 'ignore').strip()}")
    return stdout


async def synthesize_tts_wav(text: str, voice: str, output_format: str) -> bytes:
    mp3_audio = await synthesize_tts_mp3(text, voice)
    if not mp3_audio:
        return b""
    sample_rate = TTS_WAV_SAMPLE_RATE if TTS_WAV_SAMPLE_RATE > 0 else _wav_rate_from_format(output_format)
    return await _mp3_to_wav(mp3_audio, sample_rate)


async def synthesize_tts_raw(text: str, voice: str, sample_rate: int) -> bytes:
    mp3_audio = await synthesize_tts_mp3(text, voice)
    if not mp3_audio:
        return b""
    rate = sample_rate if sample_rate > 0 else 8000
    return await _mp3_to_raw(mp3_audio, rate)


async def send_stream_audio(ws: WebSocket, audio_bytes: bytes, audio_type: str = "mp3", sample_rate: int = SAMPLE_RATE):
    payload = {
        "type": "streamAudio",
        "data": {
            "audioDataType": audio_type,
            "audioData": base64.b64encode(audio_bytes).decode("ascii"),
        },
    }
    if audio_type == "raw":
        payload["data"]["sampleRate"] = int(sample_rate)
    await ws.send_text(json.dumps(payload, ensure_ascii=False))


async def cancel_task(task: Optional[asyncio.Task], label: str):
    if task and not task.done():
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass
        except Exception as e:
            LOG.warning("[warn] %s cancel exception: %s", label, e)


async def play_tts_text(ws: WebSocket, state: SessionState, text: str):
    if not text:
        return
    reply = text
    if MAX_TTS_CHARS > 0:
        reply = reply[:MAX_TTS_CHARS]
    if SERVER_TTS_ENABLED and USE_FS_TTS:
        safe_text = reply.replace("\n", " ").replace("\r", " ").replace('"', "").replace("|", " ").strip()
        if not safe_text:
            return
        if not LISTEN_DURING_TTS and not ALLOW_BARGE_IN:
            est = _estimate_tts_duration_seconds(safe_text, "mp3", b"", SAMPLE_RATE)
            if TTS_MIN_SUPPRESS_MS > 0:
                est = max(est, TTS_MIN_SUPPRESS_MS / 1000.0)
            state.suppress_until = time.time() + est + (POST_TTS_GRACE_MS / 1000.0)
            _schedule_mic_unmute(state)
        speak_arg = f"speak::{FS_TTS_ENGINE}|{FS_TTS_VOICE}|{safe_text}"
        reply_text = await _esl_api_cmd(f"uuid_broadcast {state.session_id} {speak_arg} aleg")
        if reply_text and not reply_text.startswith("+OK"):
            LOG.warning("[call %s] uuid_broadcast speak failed: %s", state.session_id, reply_text)
        return

    if not SERVER_TTS_ENABLED:
        await ws.send_text(json.dumps({"type": "text", "text": reply}, ensure_ascii=False))
        return

    LOG.info("[call %s] TTS start (voice=%s, chars=%d)", state.session_id, state.voice, len(reply))
    if TTS_BACKEND == "piper":
        try:
            audio = await synthesize_tts_piper(reply, state.language or LANG_FALLBACK)
            tts_type = "wav"
        except Exception as e:
            LOG.warning("[call %s] Piper TTS failed: %s", state.session_id, e)
            if not PIPER_FALLBACK_EDGE:
                raise
            if TTS_AUDIO_TYPE == "mp3":
                audio = await synthesize_tts_mp3(reply, state.voice)
                tts_type = "mp3"
            elif TTS_AUDIO_TYPE == "raw":
                audio = await synthesize_tts_raw(reply, state.voice, TTS_RAW_SAMPLE_RATE)
                tts_type = "raw"
            else:
                audio = await synthesize_tts_wav(reply, state.voice, EDGE_OUTPUT_FORMAT)
                tts_type = "wav"
    else:
        if TTS_AUDIO_TYPE == "mp3":
            audio = await synthesize_tts_mp3(reply, state.voice)
            tts_type = "mp3"
        elif TTS_AUDIO_TYPE == "raw":
            audio = await synthesize_tts_raw(reply, state.voice, TTS_RAW_SAMPLE_RATE)
            tts_type = "raw"
        else:
            audio = await synthesize_tts_wav(reply, state.voice, EDGE_OUTPUT_FORMAT)
            tts_type = "wav"

    if not audio:
        LOG.warning("[call %s] TTS produced no audio", state.session_id)
        return
    LOG.info("[call %s] TTS done (bytes=%d) -> streamAudio(%s)", state.session_id, len(audio), tts_type)
    sample_rate = TTS_RAW_SAMPLE_RATE if tts_type == "raw" else SAMPLE_RATE
    if not LISTEN_DURING_TTS and not ALLOW_BARGE_IN:
        est = _estimate_tts_duration_seconds(reply, tts_type, audio, sample_rate)
        if TTS_MIN_SUPPRESS_MS > 0:
            est = max(est, TTS_MIN_SUPPRESS_MS / 1000.0)
        state.suppress_until = time.time() + est + (POST_TTS_GRACE_MS / 1000.0)
        _schedule_mic_unmute(state)
    state.speaking_task = asyncio.create_task(
        send_stream_audio(ws, audio, audio_type=tts_type, sample_rate=sample_rate)
    )
    await state.speaking_task
    state.speaking_task = None

async def handle_barge_in(state: SessionState):
    # Avoid uuid_break here; it can break the dialplan sleep/park and hang up the call.
    # Only stop in-progress TTS playback; do not cancel LLM.
    state.interrupted = True
    await cancel_task(state.speaking_task, "tts")
    state.speaking_task = None


async def run_llm_and_tts(
    ws: WebSocket,
    state: SessionState,
    user_text: str,
    turn_id: int,
    turn_start: Optional[float] = None,
):
    try:
        LOG.info("[call %s] LLM request: %s", state.session_id, _clip(user_text, 160))
        t2 = time.perf_counter()
        reply = await ollama_reply(state, user_text)
        t3 = time.perf_counter()
        LOG.info("[call %s] LLM %.0f ms", state.session_id, (t3 - t2) * 1000)
        if not reply:
            LOG.info("[call %s] LLM empty reply", state.session_id)
            return
        # If a newer utterance started, drop this reply.
        if state.turn_id != turn_id:
            LOG.info("[call %s] LLM reply dropped (superseded)", state.session_id)
            return
        LOG.info("[call %s] LLM reply: %s", state.session_id, _clip(reply, 200))
        t4 = time.perf_counter()
        await play_tts_text(ws, state, reply)
        t5 = time.perf_counter()
        if turn_start is not None:
            LOG.info(
                "[call %s] TTS %.0f ms | TURN %.0f ms",
                state.session_id,
                (t5 - t4) * 1000,
                (t5 - turn_start) * 1000,
            )
        else:
            LOG.info("[call %s] TTS %.0f ms", state.session_id, (t5 - t4) * 1000)
    except asyncio.CancelledError:
        raise
    except Exception as e:
        LOG.error("LLM/TTS error: %s", e)
    finally:
        # Re-enable listening after LLM/TTS completes.
        state.block_listen = False


async def process_utterance(ws: WebSocket, state: SessionState, utterance_pcm16: bytes):
    # Do not uuid_break here; it can break the dialplan sleep/park and hang up the call.
    state.interrupted = False
    state.turn_id += 1
    turn_id = state.turn_id
    t_turn0 = time.perf_counter()
    try:
        if len(utterance_pcm16) < MIN_UTTERANCE_BYTES:
            LOG.info("[call %s] Utterance too short (%d bytes), skipped", state.session_id, len(utterance_pcm16))
            return

        if MIN_UTTERANCE_RMS > 0:
            rms = pcm16_rms(utterance_pcm16)
            if rms < MIN_UTTERANCE_RMS:
                LOG.info("[call %s] Utterance too quiet (rms=%.4f), skipped", state.session_id, rms)
                return

        try:
            loop = asyncio.get_running_loop()
            lang_hint = state.language if state.language_locked and state.language else ""
            model = whisper_kh if (state.language_locked and state.language == "km" and whisper_kh) else whisper_default
            utt_seconds = len(utterance_pcm16) / (SAMPLE_RATE * SAMPLE_WIDTH)
            LOG.info("[call %s] Utterance %.2fs", state.session_id, utt_seconds)
            t0 = time.perf_counter()
            async with STT_SEMAPHORE:
                text, avg_logprob, no_speech_prob, detected_lang, lang_prob = await loop.run_in_executor(
                    None, stt_whisper_from_pcm16, utterance_pcm16, lang_hint, model
                )
            t1 = time.perf_counter()
            LOG.info("[call %s] STT %.0f ms", state.session_id, (t1 - t0) * 1000)
        except Exception as e:
            LOG.error("STT error: %s", e)
            return

        if detected_lang:
            if not state.language_locked:
                if STT_LANGUAGE and STT_LANGUAGE.lower() not in ("auto", "none", ""):
                    state.language = detected_lang
                    state.language_prob = lang_prob
                    state.language_locked = True
                elif lang_prob >= LANG_LOCK_MIN_PROB:
                    state.language = detected_lang
                    state.language_prob = lang_prob
                    state.language_locked = True
                    if state.language == "km" and whisper_kh:
                        LOG.info(
                            "[call %s] Switching to Khmer model for STT",
                            state.session_id,
                        )
                else:
                    LOG.info(
                        "[call %s] Language detect low confidence: %s (p=%.2f) -> ignore",
                        state.session_id,
                        detected_lang,
                        lang_prob,
                    )
            if state.language_locked and state.language:
                new_voice = _voice_for_lang(state.language)
                if new_voice != state.voice:
                    state.voice = new_voice
                LOG.info(
                    "[call %s] Language locked: %s (p=%.2f) -> voice=%s",
                    state.session_id,
                    state.language,
                    state.language_prob,
                    state.voice,
                )

        if not text.strip():
            LOG.info("[call %s] STT empty transcript", state.session_id)
            return

        if IVR_SPEECH_ENABLED:
            lang_choice = _match_ivr_speech_choice(text)
            if lang_choice:
                LOG.info("[call %s] Speech IVR detected language: %s", state.session_id, lang_choice)
                await _handle_language_switch(state, lang_choice)
                return

        lang_for_thresholds = (state.language or detected_lang or "").lower()
        use_non_en = bool(lang_for_thresholds) and not lang_for_thresholds.startswith("en")
        min_words = MIN_WORDS_NON_EN if use_non_en else MIN_WORDS
        min_avg_logprob = MIN_AVG_LOGPROB_NON_EN if use_non_en else MIN_AVG_LOGPROB
        max_no_speech_prob = MAX_NO_SPEECH_PROB_NON_EN if use_non_en else MAX_NO_SPEECH_PROB

        if no_speech_prob > max_no_speech_prob:
            LOG.info(
                "[call %s] STT skipped (no_speech_prob=%.2f > %.2f)",
                state.session_id,
                no_speech_prob,
                max_no_speech_prob,
            )
            return

        if avg_logprob < min_avg_logprob:
            LOG.info(
                "[call %s] STT skipped (avg_logprob=%.2f < %.2f)",
                state.session_id,
                avg_logprob,
                min_avg_logprob,
            )
            return

        word_count = len([w for w in text.strip().split() if w])
        if word_count < min_words:
            LOG.info(
                "[call %s] STT skipped (words=%d < %d)",
                state.session_id,
                word_count,
                min_words,
            )
            return

        text = _normalize_company_terms(text)

        # Merge with previous short utterance if it arrived very recently.
        text_to_use = text
        now = time.time()
        if state.last_user_text:
            delta = now - state.last_user_time
            prev_words = len([w for w in state.last_user_text.split() if w])
            if delta <= (MERGE_WINDOW_MS / 1000.0) and prev_words <= MERGE_PREV_WORDS_MAX:
                # Remove last user entry from history to avoid duplication.
                if state.history and state.history[-1].get("role") == "user":
                    if state.history[-1].get("content") == state.last_user_text:
                        state.history.pop()
                merged = f"{state.last_user_text} {text}".strip()
                if len(merged) > MERGE_MAX_CHARS:
                    merged = merged[-MERGE_MAX_CHARS:]
                text_to_use = merged
                LOG.info("[call %s] STT merged: %s", state.session_id, _clip(text_to_use, 200))

        state.last_user_text = text_to_use
        state.last_user_time = now

        LOG.info(
            "[call %s] STT: %s (avg_logprob=%.2f no_speech_prob=%.2f)",
            state.session_id,
            _clip(text_to_use, 200),
            avg_logprob,
            no_speech_prob,
        )
        state.llm_task = asyncio.create_task(run_llm_and_tts(ws, state, text_to_use, turn_id, t_turn0))
        try:
            await state.llm_task
        except asyncio.CancelledError:
            pass
        finally:
            state.llm_task = None
    finally:
        # Always re-enable listening so short/empty transcripts don't lock the session.
        state.block_listen = False


async def utterance_worker(ws: WebSocket, state: SessionState, q: asyncio.Queue):
    try:
        while True:
            job = await q.get()
            if job is None:
                break
            if job.pcm16 is not None:
                await process_utterance(ws, state, job.pcm16)
    except WebSocketDisconnect:
        pass
    except Exception as e:
        LOG.error("worker error: %s", e)
    finally:
        await cancel_task(state.speaking_task, "tts")
        await cancel_task(state.llm_task, "llm")


async def enqueue_utterance(utter_q: asyncio.Queue, job: UtteranceJob):
    if utter_q.full():
        LOG.warning("utter_q full, dropping oldest")
        try:
            _ = utter_q.get_nowait()
        except Exception:
            pass
    await utter_q.put(job)


def _parse_metadata(text: str) -> dict:
    if not text:
        return {}
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        return {}


@app.websocket("/ws_fs")
async def ws_fs_endpoint(ws: WebSocket):
    await ws.accept()
    vad = webrtcvad.Vad(VAD_MODE)

    session_id = ws.query_params.get("session_id") or str(uuid.uuid4())
    state = SessionState(session_id=session_id)
    state.ws = ws
    ACTIVE_SESSIONS[state.session_id] = state
    if STT_LANGUAGE and STT_LANGUAGE.lower() not in ("auto", "none", ""):
        state.language = STT_LANGUAGE.lower()
        state.language_locked = True
        state.voice = _voice_for_lang(state.language)
        LOG.info(
            "[call %s] Language preset: %s -> voice=%s",
            state.session_id,
            state.language,
            state.voice,
        )
    elif DEFAULT_LANGUAGE:
        state.voice = _voice_for_lang(DEFAULT_LANGUAGE)
    LOG.info("[call %s] WS connected", state.session_id)

    utter_q: asyncio.Queue[Optional[UtteranceJob]] = asyncio.Queue(maxsize=2)
    state.utter_q = utter_q
    state.worker_task = asyncio.create_task(utterance_worker(ws, state, utter_q))

    if IVR_LANG_SELECT_ENABLED:
        state.lang_select_active = True
        state.lang_select_deadline = 0.0

    if GREETING_ENABLED and GREETING_TEXT or (IVR_LANG_SELECT_ENABLED and IVR_LANG_PROMPT):
        try:
            state.block_listen = True
            _set_mic_state(state, True, "greeting")
            if GREETING_ENABLED and GREETING_TEXT:
                await play_tts_text(ws, state, GREETING_TEXT)
            if IVR_LANG_SELECT_ENABLED and IVR_LANG_PROMPT:
                # Ensure prompt is spoken in default language
                if DEFAULT_LANGUAGE:
                    state.voice = _voice_for_lang(DEFAULT_LANGUAGE)
                await play_tts_text(ws, state, IVR_LANG_PROMPT)
        finally:
            state.block_listen = False

    pcm_buffer = bytearray()
    speech_buffer = bytearray()
    pre_speech_buffer = bytearray()
    in_speech = False
    silence_frames = 0
    speech_start_frames = 0
    noise_floor = float(os.getenv("NOISE_FLOOR_INIT", "0.0005"))
    max_utt_bytes = int(MAX_UTTERANCE_SECONDS * SAMPLE_RATE * SAMPLE_WIDTH)

    try:
        while True:
            try:
                msg = await ws.receive()
            except RuntimeError as e:
                # Starlette raises on receive after disconnect
                if "disconnect" in str(e).lower():
                    break
                raise

            if msg.get("text") is not None:
                meta = _parse_metadata(msg["text"])
                if meta.get("session_id"):
                    old_id = state.session_id
                    state.session_id = str(meta["session_id"])
                    if old_id != state.session_id:
                        ACTIVE_SESSIONS.pop(old_id, None)
                        ACTIVE_SESSIONS[state.session_id] = state
                    LOG.info("[call %s] Metadata session_id set", state.session_id)
                continue

            b = msg.get("bytes")
            if not b:
                continue

            pcm_buffer.extend(b)

            while len(pcm_buffer) >= FRAME_BYTES:
                frame = bytes(pcm_buffer[:FRAME_BYTES])
                del pcm_buffer[:FRAME_BYTES]

                muted_reason = ""
                if state.block_listen:
                    muted_reason = "processing"
                elif not LISTEN_DURING_TTS and state.suppress_until and time.time() < state.suppress_until:
                    muted_reason = "tts"

                if muted_reason:
                    _set_mic_state(state, True, muted_reason)
                    # LLM/TTS in progress; ignore mic input to avoid echo/false triggers.
                    in_speech = False
                    silence_frames = 0
                    speech_start_frames = 0
                    speech_buffer.clear()
                    pre_speech_buffer.clear()
                    continue
                else:
                    _set_mic_state(state, False, "")

                rms = pcm16_rms(frame)
                if not in_speech:
                    noise_floor = (NOISE_FLOOR_ALPHA * noise_floor) + ((1.0 - NOISE_FLOOR_ALPHA) * rms)
                dynamic_thresh = max(VAD_RMS_THRESHOLD, noise_floor * NOISE_FLOOR_MULT)
                is_speech = vad.is_speech(frame, SAMPLE_RATE) and rms >= dynamic_thresh

                if ALLOW_BARGE_IN and is_speech and (
                    state.speaking_task and not state.speaking_task.done()
                ):
                    await handle_barge_in(state)

                if is_speech:
                    if not in_speech:
                        speech_start_frames += 1
                        pre_speech_buffer.extend(frame)
                        if speech_start_frames < START_SPEECH_FRAMES:
                            continue
                        in_speech = True
                        silence_frames = 0
                        speech_buffer.clear()
                        speech_buffer.extend(pre_speech_buffer)
                        pre_speech_buffer.clear()
                        LOG.info("[call %s] Speech start", state.session_id)

                    speech_buffer.extend(frame)
                    silence_frames = 0

                    if len(speech_buffer) >= max_utt_bytes:
                        in_speech = False
                        utterance = bytes(speech_buffer)
                        speech_buffer.clear()
                        LOG.info("[call %s] Speech end (max_len), bytes=%d", state.session_id, len(utterance))
                        await enqueue_utterance(utter_q, UtteranceJob(pcm16=utterance))

                else:
                    speech_start_frames = 0
                    pre_speech_buffer.clear()
                    if in_speech:
                        silence_frames += 1
                        speech_buffer.extend(frame)

                        if silence_frames >= END_SILENCE_FRAMES:
                            in_speech = False
                            silence_frames = 0
                            utterance = bytes(speech_buffer)
                            speech_buffer.clear()
                            LOG.info("[call %s] Speech end (silence), bytes=%d", state.session_id, len(utterance))
                            await enqueue_utterance(utter_q, UtteranceJob(pcm16=utterance))

    except WebSocketDisconnect:
        pass
    finally:
        LOG.info("[call %s] WS disconnected", state.session_id)
        ACTIVE_SESSIONS.pop(state.session_id, None)
        try:
            await utter_q.put(None)
        except Exception:
            pass
        await cancel_task(state.worker_task, "worker")
        await cancel_task(state.speaking_task, "tts")
        await cancel_task(state.llm_task, "llm")
        await cancel_task(state.mic_unmute_task, "mic_unmute")


async def _esl_read_message(reader: asyncio.StreamReader) -> dict:
    headers = {}
    while True:
        line = await reader.readline()
        if not line:
            break
        if line in (b"\n", b"\r\n"):
            break
        decoded = line.decode("utf-8", errors="ignore").strip()
        if not decoded:
            break
        key, _, value = decoded.partition(":")
        if key:
            headers[key.strip()] = value.strip()

    body = b""
    content_len = headers.get("Content-Length")
    if content_len:
        try:
            length = int(content_len)
        except ValueError:
            length = 0
        if length > 0:
            body = await reader.readexactly(length)

    return {"headers": headers, "body": body.decode("utf-8", errors="ignore")}


def _parse_event_headers(text: str) -> dict:
    out = {}
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        key, _, value = line.partition(":")
        if key:
            k = unquote(key.strip())
            v = unquote(value.strip())
            out[k] = v
    return out


async def _esl_api_cmd(cmd: str) -> str:
    async with ESL_API_SEMAPHORE:
        reader, writer = await asyncio.open_connection(ESL_HOST, ESL_PORT)
        _ = await _esl_read_message(reader)  # auth/request
        writer.write(f"auth {ESL_PASSWORD}\n\n".encode("utf-8"))
        await writer.drain()
        _ = await _esl_read_message(reader)  # auth reply
        writer.write(f"api {cmd}\n\n".encode("utf-8"))
        await writer.drain()
        reply = await _esl_read_message(reader)  # api reply
        writer.close()
        await writer.wait_closed()
        return reply.get("headers", {}).get("Reply-Text", "")


async def _esl_playback_loop():
    backoff = 1.0
    while True:
        try:
            LOG.info("ESL listener connecting to %s:%s", ESL_HOST, ESL_PORT)
            reader, writer = await asyncio.open_connection(ESL_HOST, ESL_PORT)
            _ = await _esl_read_message(reader)  # auth/request
            writer.write(f"auth {ESL_PASSWORD}\n\n".encode("utf-8"))
            await writer.drain()
            auth_reply = await _esl_read_message(reader)
            reply_text = auth_reply.get("headers", {}).get("Reply-Text", "")
            if not reply_text.startswith("+OK"):
                raise RuntimeError(f"ESL auth failed: {reply_text or 'unknown'}")

            writer.write(b"event plain CUSTOM mod_audio_stream::play\n\n")
            writer.write(b"event plain DTMF\n\n")
            await writer.drain()
            _ = await _esl_read_message(reader)  # subscribe reply
            LOG.info("ESL subscribed to mod_audio_stream::play")

            backoff = 1.0
            while True:
                msg = await _esl_read_message(reader)
                headers = msg.get("headers", {})
                if headers.get("Content-Type") != "text/event-plain":
                    continue

                body = msg.get("body", "")
                if not body:
                    continue

                parts = body.split("\n\n", 1)
                event_headers = _parse_event_headers(parts[0])
                event_body = parts[1].strip() if len(parts) > 1 else ""

                event_name = event_headers.get("Event-Name", "")
                if event_name == "DTMF":
                    session_id = (
                        event_headers.get("Unique-ID")
                        or event_headers.get("Channel-Call-UUID")
                        or event_headers.get("Channel-UUID")
                        or event_headers.get("variable_uuid")
                    )
                    digit = event_headers.get("DTMF-Digit", "")
                    if session_id and digit:
                        state = ACTIVE_SESSIONS.get(session_id)
                        if state:
                            lang = IVR_LANG_MAP.get(digit)
                            if lang:
                                if state.lang_switch_task and not state.lang_switch_task.done():
                                    state.lang_switch_task.cancel()
                                state.lang_switch_task = asyncio.create_task(_handle_language_switch(state, lang))
                            else:
                                LOG.info(
                                    "[call %s] DTMF %s ignored (no mapping)",
                                    state.session_id,
                                    digit,
                                )
                        else:
                            LOG.info("[call unknown] DTMF %s received (no session)", digit)
                    continue

                if event_headers.get("Event-Subclass") != "mod_audio_stream::play":
                    continue

                try:
                    payload = json.loads(event_body) if event_body else {}
                except json.JSONDecodeError:
                    payload = {}

                file_path = payload.get("file")
                audio_type = payload.get("audioDataType")
                session_id = (
                    event_headers.get("Unique-ID")
                    or event_headers.get("Channel-Call-UUID")
                    or event_headers.get("Channel-UUID")
                    or event_headers.get("variable_uuid")
                )

                if audio_type == "raw":
                    LOG.info("[call %s] ESL play skipped for raw audio", session_id or "unknown")
                    continue

                if file_path and session_id:
                    LOG.info("[call %s] ESL play -> %s", session_id, file_path)
                    reply = await _esl_api_cmd(f"uuid_broadcast {session_id} {file_path} aleg")
                    if reply and not reply.startswith("+OK"):
                        LOG.warning("[call %s] uuid_broadcast failed: %s", session_id, reply)
                        await asyncio.sleep(0.2)
                        reply = await _esl_api_cmd(f"uuid_broadcast {session_id} {file_path} aleg")
                        if reply and not reply.startswith("+OK"):
                            LOG.warning("[call %s] uuid_broadcast retry failed: %s", session_id, reply)

        except asyncio.CancelledError:
            break
        except Exception as e:
            LOG.warning("ESL playback loop error: %s", e)
            await asyncio.sleep(backoff)
            backoff = min(backoff * 2, 15.0)


@app.on_event("startup")
async def on_startup():
    if ESL_ENABLED:
        global ESL_TASK
        ESL_TASK = asyncio.create_task(_esl_playback_loop())


@app.on_event("shutdown")
async def on_shutdown():
    global ESL_TASK
    if ESL_TASK and not ESL_TASK.done():
        ESL_TASK.cancel()
        try:
            await ESL_TASK
        except asyncio.CancelledError:
            pass

=== END FILE: fs_engine.py ===

=== FILE: .env ===
# fs_engine runtime settings
VAD_MODE=3
START_SPEECH_FRAMES=3
VAD_RMS_THRESHOLD=0.004
NOISE_FLOOR_MULT=4.0
NOISE_FLOOR_ALPHA=0.95
MIN_UTTERANCE_MS=800
MIN_UTTERANCE_RMS=0.0035
END_SILENCE_MS=600
MAX_UTTERANCE_SECONDS=8
MIN_WORDS=2
MIN_AVG_LOGPROB=-1.3
MAX_NO_SPEECH_PROB=0.8
MIN_WORDS_NON_EN=1
MIN_AVG_LOGPROB_NON_EN=-1.8
MAX_NO_SPEECH_PROB_NON_EN=0.95
LISTEN_DURING_TTS=true
ALLOW_BARGE_IN=true
POST_TTS_GRACE_MS=250
TTS_CHARS_PER_SECOND=16
TTS_MIN_SUPPRESS_MS=1000
LOG_LEVEL=INFO
STT_MAX_CONCURRENCY=1
WHISPER_SIZE=small
KHMER_WHISPER_PATH=
STT_LANGUAGE=en
DEFAULT_LANGUAGE=en
LANG_LOCK_MIN_PROB=0.85
LANG_VOICE_MAP=km=km-KH-SreymomNeural,hi=hi-IN-SwaraNeural,en=en-US-JennyNeural
IVR_LANG_SELECT_ENABLED=true
IVR_LANG_PROMPT=For English, press 1 or say English. For Hindi, press 2 or say Hindi.
IVR_LANG_MAP=1=en,2=hi
IVR_LANG_TIMEOUT_MS=10000
IVR_SPEECH_ENABLED=true
IVR_SPEECH_MAX_WORDS=4
IVR_SPEECH_MAP=en=english,eng | hi=hindi,हिंदी,हिन्दी
TTS_BACKEND=piper
# Piper (offline TTS)
PIPER_BIN=/home/smith/Documents/python/pipecat/piper_wrapper.sh
PIPER_MODEL_MAP=en=/home/smith/Downloads/voices-model/en_GB-cori-medium.onnx,hi=/home/smith/piper_voices/hi_IN-priyamvada-medium.onnx
PIPER_CONFIG_MAP=en=/home/smith/Downloads/voices-model/en_GB-cori-medium.onnx.json,hi=/home/smith/piper_voices/hi_IN-priyamvada-medium.onnx.json
PIPER_ESPEAK_DATA=/usr/local/share/piper/espeak-ng-data
PIPER_SPEAKER=
PIPER_FALLBACK_EDGE=true
FS_ENGINE_ENV_OVERRIDE=true
MAX_TTS_SUPPRESS_SEC=20
COMPANY_PROFILE_PATH=company_profile.json
ALLOWED_TOPICS=
OFFTOPIC_RESPONSE=I can help with BigfootMediaTech services like voice, messaging, eSIM, plans, pricing, billing, or support. How can I assist?
OFFTOPIC_RESPONSE_SHORT=Please say: voice, messaging, eSIM, plans, billing, or support.
OFFTOPIC_REPEAT_LIMIT=2
GREETING_ENABLED=true

=== END FILE: .env ===

=== FILE: company_profile.json ===
{
  "company_name": "BigfootMediaTech",
  "tagline": "Cloud communications for voice, messaging, and eSIM.",
  "services": [
    {
      "name": "Voice",
      "description": "Business voice calling, IVR, and call center features.",
      "features": [
        "Inbound/outbound calling",
        "IVR",
        "Call recording",
        "Number provisioning"
      ]
    },
    {
      "name": "Messaging",
      "description": "SMS and messaging services for customer support and alerts.",
      "features": [
        "SMS",
        "Two-way messaging",
        "Alerts and notifications"
      ]
    },
    {
      "name": "eSIM",
      "description": "eSIM connectivity for devices and IoT.",
      "features": [
        "Device activation",
        "Coverage plans",
        "Usage tracking"
      ]
    }
  ],
  "plans": [
    {
      "name": "Starter",
      "price": "TBD",
      "includes": "Basic voice + messaging"
    },
    {
      "name": "Business",
      "price": "TBD",
      "includes": "Higher usage limits + support"
    },
    {
      "name": "Enterprise",
      "price": "TBD",
      "includes": "Custom pricing + dedicated support"
    }
  ],
  "support": {
    "hours": "Mon–Fri 9am–6pm",
    "email": "support@bigfootmediatech.com",
    "phone": "TBD"
  },
  "greeting": "Hello, this is {company_name} call center. How can I help you today?"
}

=== END FILE: company_profile.json ===

=== FILE: piper_wrapper.sh ===
#!/usr/bin/env bash
DIR_LIB="/usr/local/share/piper"
export LD_LIBRARY_PATH="${DIR_LIB}${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}"
exec /usr/local/bin/piper "$@"

=== END FILE: piper_wrapper.sh ===

=== FILE: tts_edge.py ===
#!/usr/bin/env python3
import argparse
import asyncio
import os
import subprocess
import tempfile

import edge_tts


def _run_ffmpeg(mp3_path: str, wav_path: str, sample_rate: int) -> None:
    subprocess.run(
        [
            "ffmpeg",
            "-y",
            "-hide_banner",
            "-loglevel",
            "error",
            "-i",
            mp3_path,
            "-acodec",
            "pcm_s16le",
            "-ar",
            str(sample_rate),
            "-ac",
            "1",
            wav_path,
        ],
        check=True,
    )


async def _synthesize(text: str, voice: str, wav_path: str, sample_rate: int) -> None:
    if not text.strip():
        # Generate 200ms silence if text is empty.
        subprocess.run(
            [
                "ffmpeg",
                "-y",
                "-hide_banner",
                "-loglevel",
                "error",
                "-f",
                "lavfi",
                "-i",
                f"anullsrc=r={sample_rate}:cl=mono",
                "-t",
                "0.2",
                "-acodec",
                "pcm_s16le",
                wav_path,
            ],
            check=True,
        )
        return

    with tempfile.TemporaryDirectory() as tmpdir:
        mp3_path = os.path.join(tmpdir, "tts.mp3")
        communicate = edge_tts.Communicate(text=text, voice=voice)
        await communicate.save(mp3_path)
        _run_ffmpeg(mp3_path, wav_path, sample_rate)


def main() -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument("--voice", default=os.getenv("TTS_VOICE", "en-US-JennyNeural"))
    parser.add_argument("--outfile", required=True)
    parser.add_argument("--rate", default="0")
    parser.add_argument("--volume", default="0")
    parser.add_argument("--text", nargs="+", required=True)
    args = parser.parse_args()

    text = " ".join(args.text).strip()
    sample_rate = int(os.getenv("TTS_WAV_SAMPLE_RATE", "8000"))
    asyncio.run(_synthesize(text, args.voice, args.outfile, sample_rate))
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

=== END FILE: tts_edge.py ===

=== FILE: tts_commandline.conf.xml ===
<configuration name="tts_commandline.conf" description="Command Line TTS">
  <settings>
    <param name="command" value="/usr/local/bin/tts_edge.py --voice ${voice} --outfile ${file} --text &quot;${text}&quot;"/>
  </settings>
</configuration>

=== END FILE: tts_commandline.conf.xml ===

=== FILE: Dockerfile.freeswitch ===
FROM debian:bookworm

ENV DEBIAN_FRONTEND=noninteractive
ENV FS_PREFIX=/usr/local/freeswitch
ENV PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:/usr/local/lib64/pkgconfig:/usr/lib/x86_64-linux-gnu/pkgconfig
ENV PATH="${FS_PREFIX}/bin:${PATH}"

RUN apt-get update && apt-get install -y \
    git build-essential autoconf automake libtool libtool-bin pkg-config cmake \
    nasm yasm \
    libssl-dev libcurl4-openssl-dev libspeex-dev libspeexdsp-dev zlib1g-dev libevent-dev \
    libedit-dev libsqlite3-dev libpcre3-dev libpcre2-dev libldns-dev libopus-dev libsndfile1-dev \
    libjpeg-dev libtiff-dev libavformat-dev libswscale-dev libavcodec-dev libavutil-dev \
    liblua5.2-dev libpq-dev uuid-dev libwebsockets-dev ca-certificates \
    libshout3-dev libmpg123-dev libogg-dev libvorbis-dev libmp3lame-dev \
    python3 python3-pip ffmpeg \
  && rm -rf /var/lib/apt/lists/*

RUN pip3 install --no-cache-dir --break-system-packages edge-tts

# Build sofia-sip
RUN git clone --depth 1 https://github.com/freeswitch/sofia-sip.git /usr/src/sofia-sip \
 && cd /usr/src/sofia-sip \
 && ./bootstrap.sh \
 && ./configure --prefix=/usr/local \
 && make -j"$(nproc)" \
 && make install \
 && ldconfig

# Build spandsp3
RUN git clone --depth 1 https://github.com/freeswitch/spandsp.git /usr/src/spandsp \
 && cd /usr/src/spandsp \
 && ./bootstrap.sh \
 && ./configure \
 && make -j"$(nproc)" \
 && make install \
 && ldconfig

# Build FreeSWITCH
RUN git clone --depth 1 https://github.com/signalwire/freeswitch.git /usr/src/freeswitch
WORKDIR /usr/src/freeswitch

RUN ./bootstrap.sh -j \
 && if [ -f modules.conf.in ]; then cp modules.conf.in modules.conf; \
    elif [ -f build/modules.conf.in ]; then cp build/modules.conf.in modules.conf; \
    fi \
 && sed -i -e '/mod_verto/s/^/#/' -e '/mod_signalwire/s/^/#/' modules.conf \
 && sed -i -e '/mod_shout/s/^#//' modules.conf \
 && sed -i -e '/mod_tts_commandline/s/^#//' modules.conf \
 && grep -q '^mod_shout' modules.conf || echo 'mod_shout' >> modules.conf \
 && grep -q '^mod_tts_commandline' modules.conf || echo 'mod_tts_commandline' >> modules.conf \
 && PKG_CONFIG_PATH="${PKG_CONFIG_PATH}" ./configure --disable-dependency-tracking --prefix="${FS_PREFIX}" \
 && make -j"$(nproc)" \
 && make install

# Build mod_audio_stream
RUN git clone --depth 1 https://github.com/amigniter/mod_audio_stream.git /usr/src/mod_audio_stream
WORKDIR /usr/src/mod_audio_stream
RUN git submodule update --init --recursive \
 && mkdir -p build \
 && cd build \
 && PKG_CONFIG_PATH="${FS_PREFIX}/lib/pkgconfig:${PKG_CONFIG_PATH}" \
    cmake -DCMAKE_BUILD_TYPE=Release -DFS_SRC=/usr/src/freeswitch -DFS_PREFIX="${FS_PREFIX}" .. \
 && make -j"$(nproc)" \
 && make install

# Enable module (note: FreeSWITCH installs configs under etc/freeswitch)
RUN sed -i 's|</modules>|  <load module="mod_audio_stream"/>\n</modules>|' \
  "${FS_PREFIX}/etc/freeswitch/autoload_configs/modules.conf.xml"

RUN sed -i 's|</modules>|  <load module="mod_shout"/>\n</modules>|' \
  "${FS_PREFIX}/etc/freeswitch/autoload_configs/modules.conf.xml"

RUN sed -i 's|</modules>|  <load module="mod_tts_commandline"/>\n</modules>|' \
  "${FS_PREFIX}/etc/freeswitch/autoload_configs/modules.conf.xml"

COPY tts_edge.py /usr/local/bin/tts_edge.py
RUN chmod +x /usr/local/bin/tts_edge.py
COPY tts_commandline.conf.xml ${FS_PREFIX}/etc/freeswitch/autoload_configs/tts_commandline.conf.xml

CMD ["/usr/local/freeswitch/bin/freeswitch","-nonat","-nf"]

=== END FILE: Dockerfile.freeswitch ===
